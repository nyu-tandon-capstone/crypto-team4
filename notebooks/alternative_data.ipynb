{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code to scrape bitcoin related tweets"
      ],
      "metadata": {
        "id": "KFJakiDGTRJQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xiQC8dKSLNH"
      },
      "outputs": [],
      "source": [
        "#!pip install snscrape\n",
        "import pandas as pd\n",
        "import snscrape.modules.twitter as sntwitter #scrape tweets\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import dateutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_tweets(topic, start_date, end_date, daily_limit):\n",
        "    df_lst = []\n",
        "    delta = end_date - start_date\n",
        "    days = delta.days\n",
        "    \n",
        "    spam_words = ['chance', 'Follow', 'follow', 'free', 'link', 'LINK', 'bio', \\\n",
        "                  'BIO', 'Update', 'last price', 'Current {} Price'.format(topic),\\\n",
        "                  'ONLY', 'BLOOMBERG', \"report\", \"Bollinger\", 'chart', 'unlock',\\\n",
        "                  'Poll', 'live'] #filter spam tweets by banning keywords\n",
        "\n",
        "    spam_emojis = ' -👋 -🟩 -🟢 -🟥 -🔍 -📊 -⬅️ -➡️ -🤖 -🔴 -✅ -📈 -💹 -🤖 -🔻 -🖕 -📉 -⬇️ -👇🏻 -❔ '\n",
        "    for i in range(days+1):\n",
        "        tweets = []\n",
        "        start_of_day = (start_date+timedelta(i)).strftime(\"%Y-%m-%d\")\n",
        "        end_of_day =  (start_date+timedelta(i+1)).strftime(\"%Y-%m-%d\")\n",
        "        print(\"scraping \"+start_of_day)\n",
        "        query = \"({topic})\".format(topic=topic) + ' -'+' -'.join(spam_words)+' ' + spam_emojis +\\\n",
        "        \"lang:en until:{end_of_day} since:{start_of_day} -filter:links -filter:replies\"\\\n",
        "        .format(start_of_day=start_of_day, end_of_day=end_of_day)\n",
        "\n",
        "        for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "            if len(tweets) == daily_limit:\n",
        "                break\n",
        "            else:\n",
        "                second_round_spam_words = [':', '₿', 'price now', 'Price:', 'price in', 'change in', 'price at close', \\\n",
        "                                          'price today', 'was $', '|', '?', '%', 'price of bitcoin is', 'average price of', '=', \\\n",
        "                                          \"Today's Bitcoin Price $\", \"price of bitcoin on\", \"Let's find an event\", \"⇄\",\n",
        "                                          \"The latest bitcoin price is\", \"via Chain\", \"now opening\", \"bitcoin price is now\",\\\n",
        "                                          \"contact\"]\n",
        "                to_add = True\n",
        "                for spam_word in second_round_spam_words:\n",
        "                    if spam_word.lower() in tweet.content.lower():\n",
        "                        to_add = False\n",
        "                        break \n",
        "                if to_add:\n",
        "                    tweets.append([tweet.date, tweet.username, tweet.content])\n",
        "        df_lst.append(pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet']))\n",
        "    return pd.concat(df_lst)\n",
        "\n",
        "def clean_tweet(tweet): \n",
        "    tweet = re.sub('@', '', tweet)\n",
        "    tweet = re.sub('#', '', tweet)\n",
        "    tweet = re.sub('http\\S+', '', tweet)\n",
        "    tweet = re.sub('\\n', '', tweet)\n",
        "    tweet = re.sub('\\s+', ' ', tweet)\n",
        "    return tweet\n",
        "\n",
        "def get_subjectivity(tweet):\n",
        "    return TextBlob(tweet).sentiment.subjectivity\n",
        "\n",
        "def get_polarity(tweet):\n",
        "    return TextBlob(tweet).sentiment.polarity\n",
        "def convert_timezone(date):\n",
        "    return date.tz_convert('US/Eastern')"
      ],
      "metadata": {
        "id": "dmqP4aatSQ8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = datetime(2022,10,1)\n",
        "end_date = datetime(2022,10,7)\n",
        "df = scrape_tweets(\"ethereum\", start_date, end_date, 100).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "fIDKzvcrSWyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tweet'] = df.Tweet.apply(clean_tweet)\n",
        "df['subjectivity'] = df.Tweet.apply(get_subjectivity)\n",
        "df['polarity'] = df.Tweet.apply(get_polarity)"
      ],
      "metadata": {
        "id": "4hNS3uQ2SZFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.loc[~df.Tweet.str.contains('price [for|of] [Bb]itcoin [on|is]') & ~df.Tweet.str.contains('all exchanges is') \\\n",
        "           & ~df.Tweet.str.contains('=') & ~df.Tweet.str.contains(\"Today's [Bb]itcoin [Pp]rice $\") \\\n",
        "           & ~df.Tweet.str.contains(\"Let's find an event\") \\\n",
        "           & ~df.Tweet.str.contains(\"The latest bitcoin price is\") & ~df.Tweet.str.contains(\"⇄\")\\\n",
        "           & ~df.Tweet.str.contains(\"via Chain\") & ~df.Tweet.str.contains(\"price of .+ on .+ is\")\\\n",
        "           & ~df.Tweet.str.contains(\"[Mm]arket rank is\") & ~df.Tweet.str.contains(\"Now Opening\")\\\n",
        "           & ~df.Tweet.str.contains(\"[Pp]rice is now\") & ~df.Tweet.str.contains(\"裁定取引\")\\\n",
        "           & ~df.Tweet.str.contains(\"[Pp]rice of bitcoin is now\") \\\n",
        "           & ~df.Tweet.str.contains(\"[Cc]ryptocurrency [Nn]ame [Vv]olume [Pp]rice\")\\\n",
        "           & ~df.Tweet.str.contains(\"[Pp]rice of .+ is currently\") & ~df.Tweet.str.contains(\"contact\")]\n",
        "x.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "zYTuasYrSb63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.to_csv('tweets.csv')"
      ],
      "metadata": {
        "id": "a9hWn-z2Sf_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Below is the code to scrap eth related tweets"
      ],
      "metadata": {
        "id": "rymICO8qVpoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_tweets(topic, start_date, end_date, daily_limit):\n",
        "    df_lst = []\n",
        "    delta = end_date - start_date\n",
        "    days = delta.days\n",
        "    \n",
        "    spam_words = ['chance', 'Follow', 'follow', 'free', 'link', 'LINK', 'bio', \\\n",
        "                  'BIO', 'Update', 'last price', 'Current {} Price'.format(topic),\\\n",
        "                  'ONLY', 'BLOOMBERG', \"report\", \"Bollinger\", 'chart', 'unlock',\\\n",
        "                  'Poll', 'live', 'sale'] #filter spam tweets by banning keywords\n",
        "\n",
        "    spam_emojis = ' -👋 -🟩 -🟢 -🟥 -🔍 -📊 -⬅️ -➡️ -🤖 -🔴 -✅ -📈 -💹 -🤖 -📉 -⬇️ -👇🏻 -❔ -📌 -🔗 -⛽ '\n",
        "    #spam_emojis = ''\n",
        "    for i in range(days+1):\n",
        "        tweets = []\n",
        "        start_of_day = (start_date+timedelta(i)).strftime(\"%Y-%m-%d\")\n",
        "        end_of_day =  (start_date+timedelta(i+1)).strftime(\"%Y-%m-%d\")\n",
        "        print(\"scraping \"+start_of_day)\n",
        "        query = \"({topic})\".format(topic=topic) + ' -'+' -'.join(spam_words)+' ' + spam_emojis +\\\n",
        "        \"lang:en until:{end_of_day} since:{start_of_day} -filter:links -filter:replies\"\\\n",
        "        .format(start_of_day=start_of_day, end_of_day=end_of_day)\n",
        "\n",
        "        for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "            if len(tweets) == daily_limit:\n",
        "                break\n",
        "            else:\n",
        "                second_round_spam_words = ['price is', 'price now', ':', '|', '(€', '$eth', '%', 'what do you think',\\\n",
        "                                          '- $', 'price prediction is', 'hit me up', 'in comments', 'price of',\\\n",
        "                                          '$', '?', 'tell me', 'price -', 'price update', 'dm', 'check out', 'hire me',\\\n",
        "                                          ' = ', 'nftcommunity']\n",
        "                to_add = True\n",
        "                for spam_word in second_round_spam_words:\n",
        "                    if spam_word.lower() in tweet.content.lower():\n",
        "                        to_add = False\n",
        "                        break \n",
        "                if to_add:\n",
        "                    tweets.append([tweet.date, tweet.username, tweet.content])\n",
        "        daily_df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet']).drop_duplicates(subset = \"Tweet\")\n",
        "        daily_df['Tweet'] = daily_df.Tweet.apply(clean_tweet)\n",
        "        daily_df['subjectivity'] = daily_df.Tweet.apply(get_subjectivity)\n",
        "        daily_df['polarity'] = daily_df.Tweet.apply(get_polarity)\n",
        "        df_lst.append(daily_df)\n",
        "    return pd.concat(df_lst)"
      ],
      "metadata": {
        "id": "hk2YYCmHVtXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = datetime(2012,12,1)\n",
        "end_date = datetime(2022,10,7)\n",
        "df = scrape_tweets(\"ethereum\", start_date, end_date, 1000000).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "AKkKuf3gWk-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.loc[~df.Tweet.str.contains('price [for|of] [Ee]thereum [on|is]') & ~df.Tweet.str.contains('Technical Analysis') \\\n",
        "           & ~df.Tweet.str.contains(\"[Pp]rice of .+ is .+ in \") & ~df.Tweet.str.contains(\"[1.|2.|3.|4.|5.|6.|7.|8.|9.|.10.]\")\\\n",
        "           & ~df.Tweet.str.contains(\"(↑|↓)\")]\n",
        "x.reset_index(drop=True, inplace=True)\n",
        "x.to_csv('ethereum_tweets.csv')"
      ],
      "metadata": {
        "id": "JbsCqWzlWywX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-BMPuCQ1W_FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code to scrape crypto related tweets"
      ],
      "metadata": {
        "id": "s2OEqscIOGQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_tweets(topic, start_date, end_date, daily_limit):\n",
        "    df_lst = []\n",
        "    delta = end_date - start_date\n",
        "    days = delta.days\n",
        "    \n",
        "    spam_words = ['chance', 'Follow', 'follow', 'free', 'link', 'LINK', 'bio', \\\n",
        "                  'BIO', 'Update', 'last price', 'Current {} Price'.format(topic),\\\n",
        "                  'ONLY', 'BLOOMBERG', \"report\", \"Bollinger\", 'chart', 'unlock',\\\n",
        "                  'Poll', 'live', 'sale'] #filter spam tweets by banning keywords\n",
        "\n",
        "    spam_emojis = ' -🟩 -🟢 -🟥 -🔍 -📊 -⬅️ -➡️ -🤖 -🔴 -✅ -📈 -💹 -🤖 -📉 -⬇️ -👇🏻 -❔ -📌 -🔗 -▲ '\n",
        "    #spam_emojis = ''\n",
        "    for i in range(days+1):\n",
        "        tweets = []\n",
        "        start_of_day = (start_date+timedelta(i)).strftime(\"%Y-%m-%d\")\n",
        "        end_of_day =  (start_date+timedelta(i+1)).strftime(\"%Y-%m-%d\")\n",
        "        print(\"scraping \"+start_of_day)\n",
        "        query = \"({topic})\".format(topic=topic) + ' -'+' -'.join(spam_words)+' ' + spam_emojis +\\\n",
        "        \"lang:en until:{end_of_day} since:{start_of_day} -filter:links -filter:replies\"\\\n",
        "        .format(start_of_day=start_of_day, end_of_day=end_of_day)\n",
        "\n",
        "        for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
        "            if len(tweets) == daily_limit:\n",
        "                break\n",
        "            else:\n",
        "                second_round_spam_words = [':', '%', '24h', 'price is', '=', '$', 'price today in', 'price at close', '*',\\\n",
        "                                          'financial solutions', 'vEmpire', 'tradeosiann', 'following his tweet']\n",
        "                to_add = True\n",
        "                for spam_word in second_round_spam_words:\n",
        "                    if spam_word.lower() in tweet.content.lower():\n",
        "                        to_add = False\n",
        "                        break \n",
        "                if to_add:\n",
        "                    tweets.append([tweet.date, tweet.username, tweet.content])\n",
        "        daily_df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet']).drop_duplicates(subset = \"Tweet\")\n",
        "        daily_df['Tweet'] = daily_df.Tweet.apply(clean_tweet)\n",
        "        daily_df['subjectivity'] = daily_df.Tweet.apply(get_subjectivity)\n",
        "        daily_df['polarity'] = daily_df.Tweet.apply(get_polarity)\n",
        "        df_lst.append(daily_df)\n",
        "    return pd.concat(df_lst)"
      ],
      "metadata": {
        "id": "vG_hhlmROG1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = datetime(2012,12,1)\n",
        "end_date = datetime(2022,10, 7)\n",
        "df = scrape_tweets(\"crypto\", start_date, end_date, 1000).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "n-NskbPzOfBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.loc[~df.Tweet.str.contains(\"[Pp]rice of .+ is\") & ~df.Tweet.str.contains(\"[Rr]equest\") & \\\n",
        "          ~df.Tweet.str.contains(\"Some Mobile Terminal Commands that monitors Crypto-Currency marketcap/price\") & \\\n",
        "          ~df.Tweet.str.contains(\"£[0-9]+\") & ~df.Tweet.str.contains(\"follow4follow\") & ~df.Tweet.str.contains(\"followback\") &\\\n",
        "          ~df.Tweet.str.contains(\"Find BTC\") & ~df.Tweet.str.contains(\"NowPlaying\") & ~df.Tweet.str.contains(\"price for you\") &\\\n",
        "          ~df.Tweet.str.contains(\"followers\") & ~df.Tweet.str.contains('[Tt]echnical Analysis') &\n",
        "          ~df.Tweet.str.contains('[Pp]rediction[s] for 20[0-9][0-9]') & ~df.Tweet.str.contains('PULSEWAVE PRICE') &\\\n",
        "          ~df.Tweet.str.contains(\"checkout\") & ~df.Tweet.str.contains('[Dd]aily [Aa]nalysis') &\\\n",
        "          ~df.Tweet.str.contains(\"[Pp]rice [0-9].*\") & ~df.Tweet.str.contains(\"[Ww]here .* [Pp]rice of .* be on .* 20[0-9][0-9]\") &\\\n",
        "          ~df.Tweet.str.contains(\"[Pp]rice for .+ is\") & ~df.Tweet.str.contains(\"[Ww]atch this space!\") &\\\n",
        "          ~df.Tweet.str.contains(\"winners\") & ~df.Tweet.str.contains(\"Crypcore\") & ~df.Tweet.str.contains(\"Konios\")&\\\n",
        "          ~df.Tweet.str.contains(\"mana crypto\") & ~df.Tweet.str.contains(\"[Ii]ntroducing [Tt]he [Nn]ew\") &\\\n",
        "          ~df.Tweet.str.contains(\"Although the Lightning Network has experienced growth and development\") &\\\n",
        "          ~df.Tweet.str.contains(\"Ever wonder if you are getting the best price\") & ~df.Tweet.str.contains(\"HEX reward\") &\\\n",
        "          ~df.Tweet.str.contains(\"price predictions .* 20[0-9][0-9]\") & ~df.Tweet.str.contains(\"please comment\") &\\\n",
        "          ~df.Tweet.str.contains(\"[Dd][Mm]\") & ~df.Tweet.str.contains(\"[Ee]merald [Cc]rypto\") &\\\n",
        "          ~df.Tweet.str.contains(\"[Tt]ry it out\") & ~df.Tweet.str.contains(\"[Cc]heck it out\")]\n",
        "x.reset_index(drop=True, inplace=True)\n",
        "x.to_csv('crypto_tweets.csv')"
      ],
      "metadata": {
        "id": "q5xeaAugOoem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'a{b}{c}{d}'.format(b='1', c='2', d='3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XyvVnCkmQd_G",
        "outputId": "e8955c8a-0b86-45a3-9dfe-88646a6aef15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a123'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code to scrape ny times news"
      ],
      "metadata": {
        "id": "8CHLr4_QQmQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def send_request(date):\n",
        "    '''Sends a request to the NYT Archive API for given date.'''\n",
        "    base_url = 'https://api.nytimes.com/svc/archive/v1/'\n",
        "    url = base_url + '/' + date[0] + '/' + date[1] + '.json?api-key=' + ny_times_key\n",
        "    response = requests.get(url).json()\n",
        "    time.sleep(6)\n",
        "    return response\n",
        "\n",
        "def is_valid(article, date):\n",
        "    '''An article is only worth checking if it is in range, and has a headline.'''\n",
        "    is_in_range = date > start and date < end\n",
        "    has_headline = type(article['headline']) == dict and 'main' in article['headline'].keys()\n",
        "    return is_in_range and has_headline\n",
        "\n",
        "def parse_response(response):\n",
        "    '''Parses and returns response as pandas data frame.'''\n",
        "    data = {'headline': [],  \n",
        "        'date': [], \n",
        "        'doc_type': [],\n",
        "        'material_type': [],\n",
        "        'section': [],\n",
        "        'keywords': []}\n",
        "    \n",
        "    articles = response['response']['docs'] \n",
        "    for article in articles: # For each article, make sure it falls within our date range\n",
        "        date = dateutil.parser.parse(article['pub_date']).date()\n",
        "        if is_valid(article, date):\n",
        "            data['date'].append(date)\n",
        "            data['headline'].append(article['headline']['main']) \n",
        "            if 'section' in article:\n",
        "                data['section'].append(article['section_name'])\n",
        "            else:\n",
        "                data['section'].append(None)\n",
        "            data['doc_type'].append(article['document_type'])\n",
        "            if 'type_of_material' in article: \n",
        "                data['material_type'].append(article['type_of_material'])\n",
        "            else:\n",
        "                data['material_type'].append(None)\n",
        "            keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] == 'subject']\n",
        "            data['keywords'].append(keywords)\n",
        "    x = pd.DataFrame(data)\n",
        "    return x[x.headline.str.contains('[Bb]itcoin') | x.headline.str.contains('[Ee]thereum') |\\\n",
        "  x.headline.str.contains('[Cr]ypto') | x.headline.str.contains('[B]blockchain')] "
      ],
      "metadata": {
        "id": "TRRjnHqGw4ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ny_times_key='my_api_key'\n",
        "df_lst = []\n",
        "end = datetime.date(2022, 9, 30)\n",
        "start = datetime.date(2022, 9, 1)\n",
        "months_in_range = [x.split(' ') for x in pd.date_range(start, end, freq='MS').strftime(\"%Y %-m\").tolist()]\n",
        "print(months_in_range)\n",
        "for month in months_in_range:\n",
        "    respone = send_request(month)\n",
        "    df_lst.append(parse_response(respone))"
      ],
      "metadata": {
        "id": "oTHYbG8Q7cgC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
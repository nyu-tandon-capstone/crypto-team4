{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.read_hdf('../data/price/CB/BTC-USD.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1483228800</td>\n",
       "      <td>973.35</td>\n",
       "      <td>973.37</td>\n",
       "      <td>973.37</td>\n",
       "      <td>973.35</td>\n",
       "      <td>2.122048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483228980</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.36</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483229040</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.40</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.39</td>\n",
       "      <td>5.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1483229100</td>\n",
       "      <td>973.35</td>\n",
       "      <td>973.39</td>\n",
       "      <td>973.35</td>\n",
       "      <td>973.38</td>\n",
       "      <td>12.481567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1483229160</td>\n",
       "      <td>973.38</td>\n",
       "      <td>973.39</td>\n",
       "      <td>973.39</td>\n",
       "      <td>973.38</td>\n",
       "      <td>1.496540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        epoch    high     low    open   close     volume\n",
       "0  1483228800  973.35  973.37  973.37  973.35   2.122048\n",
       "3  1483228980  973.36  973.36  973.36  973.36   0.040000\n",
       "4  1483229040  973.36  973.40  973.36  973.39   5.458800\n",
       "5  1483229100  973.35  973.39  973.35  973.38  12.481567\n",
       "6  1483229160  973.38  973.39  973.39  973.38   1.496540"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price.dropna(inplace=True)\n",
    "price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_sample = price[:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeconvert(df):\n",
    "    df.set_index(pd.to_datetime(df['epoch'], unit='s'),inplace=True)\n",
    "    df.drop(columns=['epoch'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gc/miniforge3/envs/fre7773/lib/python3.8/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "price_sample = timeconvert(price_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:00:00</th>\n",
       "      <td>973.35</td>\n",
       "      <td>973.37</td>\n",
       "      <td>973.37</td>\n",
       "      <td>973.35</td>\n",
       "      <td>2.122048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:03:00</th>\n",
       "      <td>973.36</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.36</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:04:00</th>\n",
       "      <td>973.36</td>\n",
       "      <td>973.40</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.39</td>\n",
       "      <td>5.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:05:00</th>\n",
       "      <td>973.35</td>\n",
       "      <td>973.39</td>\n",
       "      <td>973.35</td>\n",
       "      <td>973.38</td>\n",
       "      <td>12.481567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:06:00</th>\n",
       "      <td>973.38</td>\n",
       "      <td>973.39</td>\n",
       "      <td>973.39</td>\n",
       "      <td>973.38</td>\n",
       "      <td>1.496540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-08 01:36:00</th>\n",
       "      <td>903.57</td>\n",
       "      <td>905.68</td>\n",
       "      <td>905.11</td>\n",
       "      <td>905.68</td>\n",
       "      <td>2.191403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-08 01:37:00</th>\n",
       "      <td>905.18</td>\n",
       "      <td>905.20</td>\n",
       "      <td>905.18</td>\n",
       "      <td>905.19</td>\n",
       "      <td>4.719580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-08 01:38:00</th>\n",
       "      <td>905.18</td>\n",
       "      <td>905.20</td>\n",
       "      <td>905.18</td>\n",
       "      <td>905.20</td>\n",
       "      <td>3.875680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-08 01:39:00</th>\n",
       "      <td>905.20</td>\n",
       "      <td>905.20</td>\n",
       "      <td>905.20</td>\n",
       "      <td>905.20</td>\n",
       "      <td>2.601930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-08 01:40:00</th>\n",
       "      <td>905.19</td>\n",
       "      <td>905.20</td>\n",
       "      <td>905.20</td>\n",
       "      <td>905.19</td>\n",
       "      <td>0.672153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       high     low    open   close     volume\n",
       "epoch                                                         \n",
       "2017-01-01 00:00:00  973.35  973.37  973.37  973.35   2.122048\n",
       "2017-01-01 00:03:00  973.36  973.36  973.36  973.36   0.040000\n",
       "2017-01-01 00:04:00  973.36  973.40  973.36  973.39   5.458800\n",
       "2017-01-01 00:05:00  973.35  973.39  973.35  973.38  12.481567\n",
       "2017-01-01 00:06:00  973.38  973.39  973.39  973.38   1.496540\n",
       "...                     ...     ...     ...     ...        ...\n",
       "2017-01-08 01:36:00  903.57  905.68  905.11  905.68   2.191403\n",
       "2017-01-08 01:37:00  905.18  905.20  905.18  905.19   4.719580\n",
       "2017-01-08 01:38:00  905.18  905.20  905.18  905.20   3.875680\n",
       "2017-01-08 01:39:00  905.20  905.20  905.20  905.20   2.601930\n",
       "2017-01-08 01:40:00  905.19  905.20  905.20  905.19   0.672153\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, h0 = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(price_sample.values)\n",
    "y = torch.rand(len(price_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def feature_label_split(df, target_col):\n",
    "    y = df[[target_col]]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y\n",
    "\n",
    "def train_val_test_split(df, target_col, test_ratio):\n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "    X, y = feature_label_split(df, target_col)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(price_sample, 'volume', 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "def get_scaler(scaler):\n",
    "    scalers = {\n",
    "        \"minmax\": MinMaxScaler,\n",
    "        \"standard\": StandardScaler,\n",
    "        \"maxabs\": MaxAbsScaler,\n",
    "        \"robust\": RobustScaler,\n",
    "    }\n",
    "    return scalers.get(scaler.lower())()\n",
    "    \n",
    "scaler = get_scaler('robust')\n",
    "\n",
    "X_train_arr = scaler.fit_transform(X_train)\n",
    "X_val_arr = scaler.transform(X_val)\n",
    "X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(y_train)\n",
    "y_val_arr = scaler.transform(y_val)\n",
    "y_test_arr = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr)\n",
    "train_targets = torch.Tensor(y_train_arr)\n",
    "val_features = torch.Tensor(X_val_arr)\n",
    "val_targets = torch.Tensor(y_val_arr)\n",
    "test_features = torch.Tensor(X_test_arr)\n",
    "test_targets = torch.Tensor(y_test_arr)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "val = TensorDataset(val_features, val_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        \"rnn\": RNNModel,\n",
    "        \"lstm\": LSTMModel,\n",
    "        \"gru\": GRUModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                batch_losses.append(loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 50 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_test)\n",
    "                predictions.append(yhat.to(device).detach().numpy())\n",
    "                values.append(y_test.to(device).detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Training loss: 2.0303\t Validation loss: 7.3071\n",
      "[2/100] Training loss: 1.9643\t Validation loss: 7.6554\n",
      "[3/100] Training loss: 1.9217\t Validation loss: 7.8821\n",
      "[4/100] Training loss: 1.9070\t Validation loss: 8.0551\n",
      "[5/100] Training loss: 1.9007\t Validation loss: 8.1621\n",
      "[6/100] Training loss: 1.8985\t Validation loss: 8.2324\n",
      "[7/100] Training loss: 1.8983\t Validation loss: 8.2749\n",
      "[8/100] Training loss: 1.8982\t Validation loss: 8.2891\n",
      "[9/100] Training loss: 1.9013\t Validation loss: 8.2861\n",
      "[10/100] Training loss: 1.8953\t Validation loss: 8.2875\n",
      "[50/100] Training loss: 1.8871\t Validation loss: 8.1312\n",
      "[100/100] Training loss: 1.8594\t Validation loss: 8.1801\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = len(X_train.columns)\n",
    "output_dim = 1\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "model_params = {'input_dim': input_dim,\n",
    "                'hidden_dim' : hidden_dim,\n",
    "                'layer_dim' : layer_dim,\n",
    "                'output_dim' : output_dim,\n",
    "                'dropout_prob' : dropout}\n",
    "\n",
    "model = get_model('lstm', model_params)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
    "opt.plot_losses()\n",
    "\n",
    "predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('fre7773')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "667a15cf0eca415c7f3afcc265622f1a9b30cc1897df766a6ba3416af631a0c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
